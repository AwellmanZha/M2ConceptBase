{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.2 Heuristic Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "word_freq_file = \"./projects/m2conceptbase/utils/word_pos_freq_train_data_230514.txt\"\n",
    "\n",
    "\n",
    "def get_word_freq_dict(filename=\"./projects/m2conceptbase/word_freq_test_data_230102.txt\"):\n",
    "    word_freq_dict = {}\n",
    "    with open(filename, mode='r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items = line.strip().split('\\t')\n",
    "            # print(items)\n",
    "            if len(items) != 3:\n",
    "                continue\n",
    "            if items[0].strip() != items[0]:\n",
    "                continue\n",
    "            if items[2] not in word_freq_dict:\n",
    "                word_freq_dict[items[2]] = {}\n",
    "                word_freq_dict[items[2]][items[0]] = int(items[1])\n",
    "            else:\n",
    "                word_freq_dict[items[2]][items[0]] = int(items[1])\n",
    "    \n",
    "    return word_freq_dict\n",
    "\n",
    "word_freq_dict = get_word_freq_dict(word_freq_file)\n",
    "# word_freq_dict\n",
    "\n",
    "for key, value in word_freq_dict.items():\n",
    "    l = sorted(value.items(), key=lambda x:x[1], reverse=True)\n",
    "    word_freq_dict[key] = {word: count for word, count in l}\n",
    "\n",
    "with open(\"word_freq_train_data_230514_v2.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_freq_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20383\n",
      "saved.\n"
     ]
    }
   ],
   "source": [
    "with open(\"word_freq_train_data_230514_v2.json\", 'r', encoding='utf-8') as f:\n",
    "    word_freq_dict = json.load(f)\n",
    "\n",
    "stopwords = {}.fromkeys([line.rstrip() for line in open('./data_122/kg_data/stopwords.txt', 'r', encoding='utf-8')])\n",
    "excluded_char = ['\\\\', '/', '?', '', '.', ';', '=', '*', '^', '>', '<', '#', '$', '@', ')', '(', '&', '丨', '%', '`', '\\t', ' ']\n",
    "for i in stopwords:\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fa5a-zA-Z]')\n",
    "    if not re.sub(pattern, '', i):\n",
    "        excluded_char.append(i)\n",
    "excluded_char = list(set(excluded_char)) \n",
    "excluded_concepts = [\"图片\", \"大图\", \"照片\", \"版\", \"们\", \"子\", \"时\", \"人名\", \"暨\", \"令人\"]\n",
    "\n",
    "# 返回True表示要删除\n",
    "def check_excluded_char(concept):\n",
    "    if len(concept) == 1 and (65 <= ord(concept) <= 90 or 97 <= ord(concept) <= 122):\n",
    "        return True\n",
    "    if concept in excluded_concepts: # 人工排除的概念\n",
    "        return True\n",
    "    if any(char in excluded_char for char in concept):   # 包含停用词\n",
    "        # print(concepts)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "cnt = 0\n",
    "word_freq_dict_new = {}\n",
    "for pos, word_freq in word_freq_dict.items():\n",
    "    word_freq_dict_new[pos] = word_freq.copy()\n",
    "    for concept, count in word_freq.items():\n",
    "        if check_excluded_char(concept):\n",
    "            cnt += 1\n",
    "            # print(concept)\n",
    "            del(word_freq_dict_new[pos][concept])\n",
    "\n",
    "print(cnt)\n",
    "\n",
    "with open(\"word_freq_train_data_230514_v3.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_freq_dict_new, f, ensure_ascii=False, indent=4)\n",
    "print(\"saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"word_freq_train_data_230514_v3.json\", 'r', encoding='utf-8') as f:\n",
    "    word_freq_dict = json.load(f)\n",
    "# word_freq_dict\n",
    "\n",
    "def is_en(s):\n",
    "    return s.encode('utf-8').isalpha()\n",
    "\n",
    "\n",
    "word_freq_dict_new = {}\n",
    "for pos, word_freq in word_freq_dict.items():\n",
    "\n",
    "    if pos == 'ng':\n",
    "        continue\n",
    "    \n",
    "    if pos == 'nrfg' or pos == 'nrt' or pos == 'nr':\n",
    "        temp_list = list(word_freq_dict[pos].items())\n",
    "        temp_list = [item for item in temp_list if len(item[0]) > 1] # 去除单字\n",
    "        word_freq_dict_new[pos] = dict(temp_list[:50])\n",
    "    \n",
    "    if pos == 'nw': # top 300\n",
    "        temp_list = list(word_freq_dict[pos].items())\n",
    "        temp_list = [item for item in temp_list if len(item[0]) > 1] # 去除单字\n",
    "        word_freq_dict_new[pos] = dict(temp_list[:300])\n",
    "        \n",
    "    if pos == 'nt':\n",
    "        temp_list = list(word_freq_dict[pos].items())\n",
    "        temp_list = [item for item in temp_list if len(item[0]) > 1] # 去除单字\n",
    "        temp_list = [item for item in temp_list if item[1] >= 400]\n",
    "        word_freq_dict_new[pos] = dict(temp_list)\n",
    "    \n",
    "    if pos == 'ns':\n",
    "        temp_list = list(word_freq_dict[pos].items())\n",
    "        temp_list = [item for item in temp_list if len(item[0]) > 1] # 去除单字\n",
    "        temp_list = [item for item in temp_list if item[1] >= 3000]\n",
    "        word_freq_dict_new[pos] = dict(temp_list)\n",
    "    \n",
    "    if pos == 'nz':\n",
    "        temp_list = list(word_freq_dict[pos].items())\n",
    "        temp_list = [item for item in temp_list if len(item[0]) > 1] # 去除单字\n",
    "        temp_list_en = [item for item in temp_list if is_en(item[0])][:50]\n",
    "        temp_list_zh = [item for item in temp_list if not is_en(item[0])]\n",
    "        word_freq_dict_new[pos] = dict(temp_list_zh + temp_list_en)\n",
    "    \n",
    "    if pos == 'n':\n",
    "        temp_list = list(word_freq_dict[pos].items())\n",
    "        temp_list = [item for item in temp_list if not is_en(item[0])]\n",
    "        word_freq_dict_new[pos] = dict(temp_list)\n",
    "    \n",
    "with open(\"word_freq_train_data_230514_v4.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_freq_dict_new, f, ensure_ascii=False, indent=4)\n",
    "print(\"saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7531\n",
      "saved.\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "with open(\"word_freq_train_data_230514_v4.json\", 'r', encoding='utf-8') as f:\n",
    "    word_freq_dict = json.load(f)\n",
    "    \n",
    "def is_en_zh_mixed(concept):\n",
    "    has_en = False\n",
    "    has_zh = False\n",
    "    \n",
    "    for char in concept:\n",
    "        if '\\u4e00' <= char <= '\\u9fa5':\n",
    "            has_zh = True\n",
    "        elif 'a' <= char <= 'z' or 'A' <= char <= 'Z':\n",
    "            has_en = True\n",
    "            \n",
    "        if has_zh and has_en:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "word_freq_dict_new = {}\n",
    "keep_list = [\n",
    "    \"t恤\",\n",
    "    \"v领\",\n",
    "    \"b超\",\n",
    "    \"k线\",\n",
    "    \"u盘\",\n",
    "    \"a股\",\n",
    "    \"polo衫\",\n",
    "    \"bb霜\",\n",
    "    \"c语言\",\n",
    "    \"cp感\",\n",
    "    \"pos机\",\n",
    "    \"up主\",\n",
    "    \"a股市场\",\n",
    "    \"led灯\",\n",
    "    \"kt板\",\n",
    "    \"t台\",\n",
    "    \"k歌\",\n",
    "    \"pc端\",\n",
    "    \"ic卡\",\n",
    "    \"x射线\",\n",
    "    \"卡拉ok\",\n",
    "    \"维生素a\",\n",
    "    \"b超单\",\n",
    "    \"c罗\",\n",
    "    \"atm机\",\n",
    "    \"led屏\",\n",
    "    \"维生素c\",\n",
    "    \"芒果tv\",\n",
    "    \"hpv疫苗\",\n",
    "    \"ph值\",\n",
    "    \"v脸\",\n",
    "    \"真人cs\",\n",
    "    \"维生素d\",\n",
    "    \"哆啦a梦\",\n",
    "    \"x战警\"\n",
    "]\n",
    "\n",
    "cnt = 0\n",
    "word_freq_dict_new = {}\n",
    "\n",
    "for pos, word_freq in word_freq_dict.items():\n",
    "    word_freq_dict_new[pos] = word_freq.copy()\n",
    "    for word, freq in word_freq.items():\n",
    "        if is_en_zh_mixed(word):\n",
    "            if word not in keep_list:\n",
    "                # print(word, freq)\n",
    "                cnt += 1\n",
    "                del(word_freq_dict_new[pos][word])\n",
    "            # if len([1 for end in [\"级\", \"型\", \"类\", \"形\", \"卷\", \"版\", \"柱\", \"皮\", \"男\", \"l\", \"资\"] if word.endswith(end)]) > 0:\n",
    "            #     # print(word, freq)\n",
    "            #     pass\n",
    "            # elif freq > 1000: \n",
    "            #     print(\"\\\"\"+word+\"\\\"\"+\",\")\n",
    "print(cnt)\n",
    "\n",
    "    \n",
    "with open(\"word_freq_train_data_230514_v5.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_freq_dict_new, f, ensure_ascii=False, indent=4)\n",
    "print(\"saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"word_freq_train_data_230514_v6.json\", 'r', encoding='utf-8') as f:\n",
    "    word_freq_dict = json.load(f)\n",
    "    \n",
    "word_freq_dict_new = {}\n",
    "\n",
    "for pos, word_freq in word_freq_dict.items():\n",
    "    word_freq_dict_new[pos] = word_freq.copy()\n",
    "    for word, freq in word_freq.items():\n",
    "        if freq < 20:\n",
    "            # print(word, freq)\n",
    "            del(word_freq_dict_new[pos][word])\n",
    "\n",
    "with open(\"word_freq_train_data_230514_v7.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_freq_dict_new, f, ensure_ascii=False, indent=4)\n",
    "print(\"saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_freq_train_data_230514_v7.json` is a word_freq file with larger frequency (20) threshod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check(s):\n",
    "    import re\n",
    "    my_re = re.compile(r'[A-Za-z]',re.S)\n",
    "    res = re.findall(my_re, s)\n",
    "    if len(res):\n",
    "        return True\n",
    "    return False\n",
    "# check(\"dasc偶奇偶\")\n",
    "s = \"aAsds\".encode('utf-8')\n",
    "s.isalpha()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(concept):\n",
    "    if len(concept) == 1 and (65 <= ord(concept) <= 90 or 97 <= ord(concept) <= 122):\n",
    "        return True\n",
    "    return False\n",
    "test(\"ab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use `word_freq_train_data_230514_v6.json` to get the final word freq file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"word_freq_train_data_230514_v6.json\", 'r', encoding='utf-8') as f:\n",
    "    word_freq_dict = json.load(f)\n",
    "    \n",
    "word_freq_dict_new = {}\n",
    "\n",
    "for pos, word_freq in word_freq_dict.items():\n",
    "    for word, freq in word_freq.items():\n",
    "        word_freq_dict_new[word] = word_freq_dict_new.get(word, 0) + freq\n",
    "\n",
    "\n",
    "li = sorted(word_freq_dict_new.items(), key=lambda x:x[1], reverse=True)\n",
    "word_freq_dict_new = {word: count for word, count in li}\n",
    "\n",
    "with open(\"word_freq_train_data_230531.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_freq_dict_new, f, ensure_ascii=False, indent=4)\n",
    "print(\"saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zzw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
