{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "m2conceptbase_concept_label_dict = {}\n",
    "for i in range(0, 16):\n",
    "    if i == 0:\n",
    "        idx = \"9980\"\n",
    "    elif i == 15:\n",
    "        idx = \"final_151780\"\n",
    "    else:\n",
    "        idx = str(i) + \"9980\"\n",
    "    concept_label_path = f\"./projects/m2conceptbase/utils/results/m2conceptbase_concept_label_{idx}.json\"    \n",
    "    with open(concept_label_path, 'r', encoding='utf-8') as f:\n",
    "        concept_label_dict = json.load(f)\n",
    "        m2conceptbase_concept_label_dict.update(concept_label_dict)\n",
    "        # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151776"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m2conceptbase_concept_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['道孚',\n",
       " '克鲁日',\n",
       " '寻麻疹',\n",
       " '子龙',\n",
       " '思迅',\n",
       " '安全度',\n",
       " '鑫成君泰',\n",
       " '余派',\n",
       " '万维',\n",
       " '汉图',\n",
       " '捡趴活',\n",
       " '三觉',\n",
       " '僧俗',\n",
       " '福乐',\n",
       " '丙通沙',\n",
       " '意雕',\n",
       " '会安',\n",
       " '伯颜',\n",
       " '第二种人',\n",
       " '胆色',\n",
       " '美神',\n",
       " '东涌',\n",
       " '水哥',\n",
       " '茶门',\n",
       " '百兽凯多',\n",
       " '慕',\n",
       " '六妙法门',\n",
       " '天闻角川',\n",
       " '蔬菜界',\n",
       " '魔兽怀旧服',\n",
       " '盎格鲁',\n",
       " '继文',\n",
       " '赛迪智库',\n",
       " '人定',\n",
       " '购房网',\n",
       " '彝红',\n",
       " '曲胡',\n",
       " '博腾',\n",
       " '甗',\n",
       " '路路通',\n",
       " '官联',\n",
       " '华素愈创',\n",
       " '心健部',\n",
       " '牛男',\n",
       " '鑫融',\n",
       " '房龙',\n",
       " '美能达',\n",
       " '吉维尼',\n",
       " '乡东',\n",
       " '余景天',\n",
       " '木儿',\n",
       " '中国新闻网',\n",
       " '摩范',\n",
       " '摩根石',\n",
       " '一光',\n",
       " '诸胡',\n",
       " '尉',\n",
       " '己酮',\n",
       " '华策',\n",
       " '黑毒',\n",
       " '乐风',\n",
       " '丁酉',\n",
       " '奕泽',\n",
       " '中青宝',\n",
       " '纳安',\n",
       " '有利网',\n",
       " '瑞达法考',\n",
       " '星条',\n",
       " '亿泰',\n",
       " '信威',\n",
       " '君诚',\n",
       " '艺儿',\n",
       " '永乐',\n",
       " '车友邦',\n",
       " '中新',\n",
       " '华易算命网',\n",
       " '粤康码',\n",
       " '拉文',\n",
       " '季平',\n",
       " '捷恩斯',\n",
       " '基利',\n",
       " '养元',\n",
       " '飞宇',\n",
       " '裕子',\n",
       " '古贝春',\n",
       " '坎宁',\n",
       " '公检法司',\n",
       " '权律二',\n",
       " '九元航空',\n",
       " '罗超',\n",
       " '心佩',\n",
       " '真德',\n",
       " '万喆',\n",
       " '秦巴',\n",
       " '数量',\n",
       " '华远',\n",
       " '人论',\n",
       " '立事',\n",
       " '都匀',\n",
       " '福贡',\n",
       " '兰格钢铁网',\n",
       " '月本',\n",
       " '古录',\n",
       " '珍珠油杏',\n",
       " '依萍',\n",
       " '磁遁',\n",
       " '子冶石瓢',\n",
       " '普诺',\n",
       " '我妹',\n",
       " '星艺',\n",
       " '飞相',\n",
       " '众源',\n",
       " '科迪',\n",
       " '田中',\n",
       " '森克',\n",
       " '伊纳宝',\n",
       " '地入',\n",
       " '天仙攻',\n",
       " '断卡',\n",
       " '海创',\n",
       " '严坤',\n",
       " '短肌',\n",
       " '九阳',\n",
       " '京涛',\n",
       " '天相',\n",
       " '猎空',\n",
       " '卡巴',\n",
       " '设计邦',\n",
       " '东箭',\n",
       " '经教',\n",
       " '丁硫克百威',\n",
       " '四明',\n",
       " '胡程',\n",
       " '巴马',\n",
       " '进爵',\n",
       " '东创',\n",
       " '孝康章',\n",
       " '酒吞',\n",
       " '新网',\n",
       " '手尾',\n",
       " '遍地',\n",
       " '度汛',\n",
       " '三泽',\n",
       " '亚光速',\n",
       " '商赛',\n",
       " '丽姐',\n",
       " '鲲鲲',\n",
       " '巴尔虎',\n",
       " '钱包君',\n",
       " '老氏',\n",
       " '妥布霉素',\n",
       " '比音勒芬',\n",
       " '贰师',\n",
       " '检察部',\n",
       " '孙辈们',\n",
       " '习堂',\n",
       " '孔家',\n",
       " '导',\n",
       " '天阳',\n",
       " '西狩',\n",
       " '余峰',\n",
       " '汤川学',\n",
       " '潮气量',\n",
       " '铭科',\n",
       " '汉巴味德',\n",
       " '木化',\n",
       " '价电子',\n",
       " '饮流',\n",
       " '怡程',\n",
       " '真境',\n",
       " '胸省',\n",
       " '好男儿',\n",
       " '艾普兰',\n",
       " '明如镜',\n",
       " '御驾亲征',\n",
       " '库兵',\n",
       " '泰民',\n",
       " '研方',\n",
       " '卡尔曼',\n",
       " '毗卢',\n",
       " '至顺',\n",
       " '万斯',\n",
       " '生耳',\n",
       " '存管',\n",
       " '沃德',\n",
       " '亚盛',\n",
       " '丫霸',\n",
       " '骨力',\n",
       " '英孚',\n",
       " '翡',\n",
       " '华懋',\n",
       " '格机',\n",
       " '泰游',\n",
       " '天玑',\n",
       " '史景',\n",
       " '两当',\n",
       " '丰谷',\n",
       " '含风',\n",
       " '落籍',\n",
       " '债息',\n",
       " '委角',\n",
       " '佳家',\n",
       " '企创',\n",
       " '我的人',\n",
       " '班淑',\n",
       " '酵色',\n",
       " '克朗',\n",
       " '长势',\n",
       " '小主',\n",
       " '富丁王',\n",
       " '常程',\n",
       " '苏群',\n",
       " '空山基',\n",
       " '创二代',\n",
       " '号号',\n",
       " '基姆',\n",
       " '科思',\n",
       " '谡',\n",
       " '采邑',\n",
       " '幼升',\n",
       " '美艺',\n",
       " '护',\n",
       " '薄命司',\n",
       " '萘普生',\n",
       " '豪迪',\n",
       " '鲜生友',\n",
       " '警友',\n",
       " '国联',\n",
       " '疏',\n",
       " '任政',\n",
       " '南网',\n",
       " '破十法',\n",
       " '宝姿',\n",
       " '夷三族',\n",
       " '拒执罪',\n",
       " '科场',\n",
       " '瑞迪',\n",
       " '老范',\n",
       " '艾斯',\n",
       " '求锤得锤',\n",
       " '臧',\n",
       " '滑精',\n",
       " '东珠',\n",
       " '丹堤',\n",
       " '反皮',\n",
       " '罗门',\n",
       " '柏科',\n",
       " '恩菲尔德',\n",
       " '爱育',\n",
       " '仙命',\n",
       " '协里',\n",
       " '鑫丰',\n",
       " '庆父',\n",
       " '煞',\n",
       " '新源',\n",
       " '申精',\n",
       " '谷德设计网',\n",
       " '息影',\n",
       " '副题',\n",
       " '长安网',\n",
       " '鱼剂',\n",
       " '河田雅史',\n",
       " '吖啶',\n",
       " '乙弗',\n",
       " '颜同',\n",
       " '淋溶',\n",
       " '会典',\n",
       " '国翠',\n",
       " '小春',\n",
       " '百隆',\n",
       " '内江',\n",
       " '西湖益联保',\n",
       " '佳邦',\n",
       " '家牛',\n",
       " '餐饮科',\n",
       " '景行',\n",
       " '云果',\n",
       " '北洋龙',\n",
       " '化龙',\n",
       " '婆妈',\n",
       " '萨巴',\n",
       " '成华',\n",
       " '吸乳',\n",
       " '母儿',\n",
       " '多特',\n",
       " '守分',\n",
       " '蕴玉',\n",
       " '努力餐',\n",
       " '会厌',\n",
       " '信联',\n",
       " '瑞弗',\n",
       " '金政基',\n",
       " '阿都',\n",
       " '常宝',\n",
       " '澳洲房产网',\n",
       " '乱丝',\n",
       " '山静',\n",
       " '招商贷',\n",
       " '多头',\n",
       " '季承',\n",
       " '库拍',\n",
       " '建信',\n",
       " '巢脾',\n",
       " '养育恩',\n",
       " '富拉尔基',\n",
       " '雷摩',\n",
       " '速比',\n",
       " '万熙',\n",
       " '春归',\n",
       " '索普',\n",
       " '中策',\n",
       " '毛焰',\n",
       " '摩比爱数学',\n",
       " '中师',\n",
       " '德天',\n",
       " '纳宝帝',\n",
       " '宏邦',\n",
       " '前三甲',\n",
       " '永明',\n",
       " '秦商',\n",
       " '巴德',\n",
       " '傲剑',\n",
       " '门极',\n",
       " '朝天',\n",
       " '现言',\n",
       " '什么鬼',\n",
       " '载涛',\n",
       " '大华',\n",
       " '布地奈德',\n",
       " '美可卓',\n",
       " '孙民',\n",
       " '场频',\n",
       " '暨新',\n",
       " '奥坎',\n",
       " '勿念',\n",
       " '广星',\n",
       " '麦昆',\n",
       " '优卡丹',\n",
       " '安博维',\n",
       " '百度健康',\n",
       " '贰臣',\n",
       " '余笛',\n",
       " '汉迪',\n",
       " '学诚',\n",
       " '问梅',\n",
       " '泰力',\n",
       " '天水在线',\n",
       " '中心圈',\n",
       " '房金',\n",
       " '惠博普',\n",
       " '能剧',\n",
       " '高鸣',\n",
       " '玩玩',\n",
       " '仝卓',\n",
       " '沐阳',\n",
       " '黎',\n",
       " '咏麟',\n",
       " '民革',\n",
       " '疫天使',\n",
       " '书者',\n",
       " '博地',\n",
       " '梦野',\n",
       " '镇安',\n",
       " '脑后',\n",
       " '衬度',\n",
       " '咆哮帝',\n",
       " '泰米',\n",
       " '研祥',\n",
       " '王步',\n",
       " '木九十',\n",
       " '劳义',\n",
       " '型臂',\n",
       " '吉珠',\n",
       " '塔猜亚',\n",
       " '浚',\n",
       " '甲状',\n",
       " '声网',\n",
       " '王度',\n",
       " '众创',\n",
       " '恩威',\n",
       " '乙',\n",
       " '雪崩能天使',\n",
       " '额驸',\n",
       " '黑定',\n",
       " '游地',\n",
       " '冠泰',\n",
       " '爱驰',\n",
       " '布泉',\n",
       " '瓦莱',\n",
       " '蓝格',\n",
       " '妈妈生',\n",
       " '普约尔',\n",
       " '条基',\n",
       " '齿状突',\n",
       " '普斯卡什',\n",
       " '獭祭',\n",
       " '巧克力囊肿',\n",
       " '咸宁',\n",
       " '超威',\n",
       " '嵌顿疝',\n",
       " '菲克',\n",
       " '二次过街',\n",
       " '兴',\n",
       " '淑华',\n",
       " '内人',\n",
       " '管辖权异议',\n",
       " '颗颗',\n",
       " '达特茅斯',\n",
       " '昂达',\n",
       " '家长群',\n",
       " '一网',\n",
       " '古元',\n",
       " '木易',\n",
       " '観',\n",
       " '昌发',\n",
       " '星落',\n",
       " '赢稷',\n",
       " '中骏云',\n",
       " '华光',\n",
       " '情战',\n",
       " '黑金沙',\n",
       " '黑诊所',\n",
       " '元淳',\n",
       " '碧痕',\n",
       " '大威',\n",
       " '周教授',\n",
       " '黄品',\n",
       " '满宠',\n",
       " '二法',\n",
       " '华力',\n",
       " '平保龙',\n",
       " '龙哥',\n",
       " '员额',\n",
       " 'psd',\n",
       " '风风',\n",
       " '病娇系',\n",
       " '小波',\n",
       " '苏迪',\n",
       " '文峰',\n",
       " '麦坤',\n",
       " '紫儿',\n",
       " '全品学',\n",
       " '塔塔',\n",
       " '海普诺凯',\n",
       " '柴妹',\n",
       " '深业上城',\n",
       " '巧言石',\n",
       " '郄穴',\n",
       " '普伊格',\n",
       " '黄腊丁',\n",
       " '镇南',\n",
       " '关文',\n",
       " '警心',\n",
       " '韩姨',\n",
       " '澳宝',\n",
       " '侨城一号',\n",
       " '高仙',\n",
       " '途易',\n",
       " '联盛',\n",
       " '众汇',\n",
       " '华晶',\n",
       " '蓝湖',\n",
       " '动客',\n",
       " '于民',\n",
       " '甲地孕酮',\n",
       " '悬腕',\n",
       " '胃俞',\n",
       " '泊美',\n",
       " '尤因',\n",
       " '星妹',\n",
       " '国固',\n",
       " '莪术',\n",
       " '原耽',\n",
       " '小雄',\n",
       " '白左',\n",
       " '欧博',\n",
       " '万平口',\n",
       " '库尔德',\n",
       " '图巴',\n",
       " '星播客',\n",
       " '气蒸',\n",
       " '李势',\n",
       " '动视云',\n",
       " '范明',\n",
       " '男神收割机',\n",
       " '灌云',\n",
       " '安农',\n",
       " '君庭',\n",
       " '世亲',\n",
       " '陈数',\n",
       " '韩东',\n",
       " '众家',\n",
       " '恋爱脑',\n",
       " '剖',\n",
       " '奉承话',\n",
       " '老崔',\n",
       " '磨憨',\n",
       " '博赛',\n",
       " '帕森',\n",
       " '天奴',\n",
       " '大夫第',\n",
       " '八友',\n",
       " '一手房',\n",
       " '君汇世家',\n",
       " '四大恶人',\n",
       " '中牟',\n",
       " '狼迹',\n",
       " '乐肤洁',\n",
       " '国办',\n",
       " '钉钉',\n",
       " '暗夜行者',\n",
       " '侨鑫',\n",
       " '泡粉',\n",
       " '美作',\n",
       " '影力',\n",
       " '文爷',\n",
       " '塔影',\n",
       " '科尔尼',\n",
       " '桂城',\n",
       " '八校',\n",
       " '乜',\n",
       " '麦克法兰',\n",
       " '油边',\n",
       " '范范',\n",
       " '绿端',\n",
       " '百子',\n",
       " '微点',\n",
       " '财秘',\n",
       " '五事',\n",
       " '鸿山',\n",
       " '小可人',\n",
       " '夏朗',\n",
       " '妮基',\n",
       " '方得',\n",
       " '电子科',\n",
       " '金豪',\n",
       " '龙村',\n",
       " '天公生',\n",
       " '乐卓',\n",
       " '云筑网',\n",
       " '小昭',\n",
       " '专修科',\n",
       " '茶意',\n",
       " '美妆网',\n",
       " '我的青春期',\n",
       " '表侄',\n",
       " '军职在线',\n",
       " '多布',\n",
       " '班尼路',\n",
       " '快挂',\n",
       " '小伴',\n",
       " '星然',\n",
       " '长威',\n",
       " '程志',\n",
       " '云麓',\n",
       " '纽曼',\n",
       " '德保',\n",
       " '新鉴',\n",
       " '环家',\n",
       " '二氢钾',\n",
       " '海明',\n",
       " '叔云',\n",
       " '奶坤',\n",
       " '思伤',\n",
       " '阔气',\n",
       " '融金宝',\n",
       " '福医药',\n",
       " '艾兰得',\n",
       " '中债',\n",
       " '产学研',\n",
       " '此书',\n",
       " '炸膛',\n",
       " '梵云',\n",
       " '令迦',\n",
       " '思金',\n",
       " '飞文',\n",
       " '小汤',\n",
       " '蛋王',\n",
       " '万洋',\n",
       " '铜公',\n",
       " '大魔',\n",
       " '瓷松',\n",
       " '征东',\n",
       " '阿瑞匹坦',\n",
       " '百顺',\n",
       " '三业',\n",
       " '莱斯',\n",
       " '可立克',\n",
       " '元二',\n",
       " '优亲',\n",
       " '冬子',\n",
       " '黑毛痣',\n",
       " '法眼宗',\n",
       " '搏克',\n",
       " '沙贝',\n",
       " '本德',\n",
       " '阴跷脉',\n",
       " '万托林',\n",
       " '搭界',\n",
       " '文涛',\n",
       " '天沐',\n",
       " '玖誉',\n",
       " '玉牛',\n",
       " '萃',\n",
       " '太狮',\n",
       " '申不害',\n",
       " '增募资',\n",
       " '负屃',\n",
       " '敖东',\n",
       " '凉血',\n",
       " '群喜',\n",
       " '口德',\n",
       " '电积',\n",
       " '动视',\n",
       " '雷瓦',\n",
       " '海兰信',\n",
       " '宋玺',\n",
       " '赛绩鸽',\n",
       " '诸葛瞻',\n",
       " '辞源',\n",
       " '项目名称',\n",
       " '四中全会',\n",
       " '金理',\n",
       " '全选',\n",
       " '文淇',\n",
       " '业之峰',\n",
       " '寿郎',\n",
       " '博兰',\n",
       " '中维',\n",
       " '卫生人才网',\n",
       " '云实',\n",
       " '宣科',\n",
       " '云君',\n",
       " '灵种',\n",
       " '半江',\n",
       " '瑞拉',\n",
       " '阿峰',\n",
       " '雷童',\n",
       " '屯垦戍边',\n",
       " '方科',\n",
       " '华升',\n",
       " '少冲',\n",
       " '鲨客',\n",
       " '课儿',\n",
       " '小妈',\n",
       " '莱夫',\n",
       " '正向',\n",
       " '艾比客',\n",
       " '唐人',\n",
       " '羽臣',\n",
       " '六城',\n",
       " '辅公祏',\n",
       " '隐翅虫皮炎',\n",
       " '涂毒',\n",
       " '白茯苓丁',\n",
       " '勤哲',\n",
       " '喜蜜',\n",
       " '华西秋雨',\n",
       " '丑帅',\n",
       " '牟丛',\n",
       " '辛',\n",
       " '名网',\n",
       " '摩摩哒',\n",
       " '万融',\n",
       " '博卡',\n",
       " '双代会',\n",
       " '里水',\n",
       " '烤电',\n",
       " '脾阴虚',\n",
       " '高道',\n",
       " '水果肉',\n",
       " '汤泽',\n",
       " '万腾',\n",
       " '丙察察',\n",
       " '聚维酮',\n",
       " '成济',\n",
       " '直通春晚',\n",
       " '彩经网',\n",
       " '乐滋',\n",
       " '西司',\n",
       " '来福',\n",
       " '京式',\n",
       " '锂锰',\n",
       " '文军',\n",
       " '疑冢',\n",
       " '儒之堂',\n",
       " '生田',\n",
       " '裕丰',\n",
       " '悦都',\n",
       " '樱空',\n",
       " '移项',\n",
       " '冼',\n",
       " '现货万代',\n",
       " '卡友',\n",
       " '园地产',\n",
       " '倍径',\n",
       " '尚谷',\n",
       " '微号',\n",
       " '美森',\n",
       " '去甲',\n",
       " '彩练',\n",
       " '选委',\n",
       " '趣闲赚',\n",
       " '义弟',\n",
       " '笔札',\n",
       " '学名',\n",
       " '索面',\n",
       " '三庭五眼',\n",
       " '国家知识',\n",
       " '小三阳',\n",
       " '数企',\n",
       " '单群',\n",
       " '做空',\n",
       " '顺时',\n",
       " '道格',\n",
       " '普莱斯',\n",
       " '华城',\n",
       " '好汉坡',\n",
       " '丽江篇',\n",
       " '星姐',\n",
       " '总章',\n",
       " '吃货们',\n",
       " '科波',\n",
       " '龙氏',\n",
       " '恒鑫',\n",
       " '西瓦',\n",
       " '玄月',\n",
       " '天波府',\n",
       " '楼生',\n",
       " '光孤子',\n",
       " '四力',\n",
       " '华南',\n",
       " '信元发育宝',\n",
       " '蓝冰',\n",
       " '快龙',\n",
       " '胃素',\n",
       " '木油',\n",
       " '益齿',\n",
       " '腐文',\n",
       " '圉',\n",
       " '吡嘧磺隆',\n",
       " '多龙',\n",
       " '立达信',\n",
       " '海天',\n",
       " '昕锐',\n",
       " '青红',\n",
       " '狄人',\n",
       " '做局',\n",
       " '飘零',\n",
       " '驻穗',\n",
       " '阿旗',\n",
       " '苗厨',\n",
       " '蜗蜗',\n",
       " '衰衰',\n",
       " '正大和尚',\n",
       " '一号',\n",
       " '黄济',\n",
       " '修丽可',\n",
       " '国隆',\n",
       " '春蕾班',\n",
       " '瑞迈',\n",
       " '诺泰',\n",
       " '乐康膏',\n",
       " '歌林',\n",
       " '波本',\n",
       " 'pptx',\n",
       " '骨康',\n",
       " '作人',\n",
       " '企汇',\n",
       " '咪咪',\n",
       " '督师府',\n",
       " '海魂',\n",
       " '靓颖',\n",
       " '朝晖',\n",
       " '艾美',\n",
       " '四郎',\n",
       " '周震',\n",
       " '局务',\n",
       " '血项',\n",
       " '窝里',\n",
       " '飞亚达',\n",
       " '姐',\n",
       " '萨尔瓦',\n",
       " '那个女人',\n",
       " '钦',\n",
       " '金泰',\n",
       " '季泉',\n",
       " '乙流',\n",
       " '杨路',\n",
       " '美世',\n",
       " '中情',\n",
       " '嘉顺',\n",
       " '王业',\n",
       " '拉达尼瓦',\n",
       " '码云',\n",
       " '宏艺',\n",
       " '链人',\n",
       " '深明',\n",
       " '义拍',\n",
       " '企翼网',\n",
       " '王淑',\n",
       " '庄行',\n",
       " '自生桥',\n",
       " '高腾',\n",
       " '贺文',\n",
       " '唯德',\n",
       " '里克斯',\n",
       " '精华录',\n",
       " '中病',\n",
       " '网销宝',\n",
       " '学台',\n",
       " '由龙',\n",
       " '义方',\n",
       " '安次',\n",
       " '梅核气',\n",
       " '精道',\n",
       " '正林',\n",
       " '日丰管',\n",
       " '衣云鹤',\n",
       " '余文',\n",
       " '空孕囊',\n",
       " '隐婚甜妻',\n",
       " '民康',\n",
       " '聪贝',\n",
       " '英主',\n",
       " '大傻',\n",
       " '良叔',\n",
       " '三浦友和',\n",
       " '我的天',\n",
       " '小星',\n",
       " '国开',\n",
       " '伊通',\n",
       " '神经根',\n",
       " '桑',\n",
       " '可元',\n",
       " '思客',\n",
       " '大情',\n",
       " '职友集',\n",
       " '麻生',\n",
       " '宾格',\n",
       " '法尔曼',\n",
       " '神友',\n",
       " '历官',\n",
       " '博众',\n",
       " '德夯',\n",
       " '我师',\n",
       " '费斯托',\n",
       " '桃子姐',\n",
       " '疾管署',\n",
       " '诺恒宇光',\n",
       " '思慕雪',\n",
       " '傻乐',\n",
       " '微码',\n",
       " '锂硫',\n",
       " '沼液',\n",
       " '扁方',\n",
       " '作弊码',\n",
       " '本安型',\n",
       " '秦权壶',\n",
       " '六味',\n",
       " '葛饰',\n",
       " '号型',\n",
       " '爱老婆',\n",
       " '电母',\n",
       " '马天尼',\n",
       " '玩命',\n",
       " '欧文斯',\n",
       " '绘生',\n",
       " '关小',\n",
       " '住交会',\n",
       " '蒙特',\n",
       " '白际',\n",
       " '文金',\n",
       " '词论',\n",
       " '黄铭',\n",
       " '第纳尔',\n",
       " '忻州网',\n",
       " '敦豪',\n",
       " '卡司',\n",
       " '押司',\n",
       " '三本生',\n",
       " '朗',\n",
       " '吴地',\n",
       " '冈田',\n",
       " '柘',\n",
       " '视帝',\n",
       " '穆林',\n",
       " '行素',\n",
       " '博卡拉',\n",
       " '沙发布',\n",
       " '同力',\n",
       " '谏大夫',\n",
       " '克虏伯',\n",
       " '名族',\n",
       " '淋失',\n",
       " '艺点',\n",
       " '小圣',\n",
       " '九虎',\n",
       " '东关',\n",
       " '航嘉',\n",
       " '峰峰',\n",
       " '哲宇',\n",
       " '呆宝静',\n",
       " '博声',\n",
       " '走狗烹',\n",
       " '骨髓象',\n",
       " '华新',\n",
       " '电功',\n",
       " '天津网',\n",
       " '帝业',\n",
       " '亚科',\n",
       " '书仁',\n",
       " '宿迁网',\n",
       " '斯太尔',\n",
       " '一水',\n",
       " '州治',\n",
       " '恒益',\n",
       " '进典',\n",
       " '世通',\n",
       " '京基百纳',\n",
       " '金影',\n",
       " '肌酐',\n",
       " '资村',\n",
       " '城纪',\n",
       " '易仓',\n",
       " '康美',\n",
       " '潜标',\n",
       " '新夏朗',\n",
       " '郭子',\n",
       " '锚段',\n",
       " '艾粄',\n",
       " '名森',\n",
       " '创始人程',\n",
       " '容志行',\n",
       " '国服瑶',\n",
       " '收肌',\n",
       " '兵兵',\n",
       " '切利尼',\n",
       " '汾',\n",
       " '巴乌',\n",
       " '借雨',\n",
       " '横水',\n",
       " '转奶',\n",
       " '崂',\n",
       " '金秦禹',\n",
       " '汉能',\n",
       " '家乐',\n",
       " '余心',\n",
       " '刊',\n",
       " '高宁',\n",
       " '舜帝',\n",
       " '沙雅',\n",
       " '白钟元',\n",
       " '书愤',\n",
       " '耄耋图',\n",
       " '载勋',\n",
       " '逸墅',\n",
       " '火记',\n",
       " '金维',\n",
       " '证网',\n",
       " '中山医',\n",
       " '师相',\n",
       " '檀悦豪生',\n",
       " '易商通',\n",
       " '莱蔻',\n",
       " '慕课室',\n",
       " '马街书会',\n",
       " '仙缘',\n",
       " '高群',\n",
       " '流花道',\n",
       " '迪亚比',\n",
       " '龙湫',\n",
       " '京成一品',\n",
       " '际',\n",
       " '非人哉',\n",
       " '觉姆',\n",
       " '振型',\n",
       " '板板',\n",
       " '味甘',\n",
       " '继发性',\n",
       " '牙菌',\n",
       " '虫螨腈',\n",
       " '海目星',\n",
       " '宝林',\n",
       " '农融',\n",
       " '东户',\n",
       " '余强',\n",
       " '奥美',\n",
       " '富瑞',\n",
       " '众投邦',\n",
       " '赛弗',\n",
       " '恩度',\n",
       " '瑞雯',\n",
       " '圣虫',\n",
       " '达音科',\n",
       " '群仙',\n",
       " '菲尔特',\n",
       " '纳克萨玛斯',\n",
       " '考妣',\n",
       " '余凯',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_concepts = [c for c, l in m2conceptbase_concept_label_dict.items() if l == 0]\n",
    "abstract_concepts = [c for c, l in m2conceptbase_concept_label_dict.items() if l == 1]\n",
    "extra_concepts = [c for c, l in m2conceptbase_concept_label_dict.items() if l == 2]\n",
    "extra_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105472 41722 4582 151776\n"
     ]
    }
   ],
   "source": [
    "concrete_num = sum([1 for c, l in m2conceptbase_concept_label_dict.items() if l == 0])\n",
    "abstract_num = sum([1 for c, l in m2conceptbase_concept_label_dict.items() if l == 1])\n",
    "extra_num = sum([1 for c, l in m2conceptbase_concept_label_dict.items() if l == 2])\n",
    "print(concrete_num, abstract_num, extra_num, concrete_num + abstract_num +extra_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing A: 100%|██████████| 1780/1780 [58:20<00:00,  1.97s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing B: 100%|██████████| 8793/8793 [5:18:13<00:00,  2.17s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing C: 100%|██████████| 9302/9302 [5:30:06<00:00,  2.13s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing D: 100%|██████████| 10842/10842 [7:01:56<00:00,  2.34s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing E: 100%|██████████| 960/960 [35:10<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing F: 100%|██████████| 6637/6637 [1:02:15<00:00,  1.78it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing G: 100%|██████████| 9519/9519 [03:25<00:00, 46.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing H: 100%|██████████| 11389/11389 [04:13<00:00, 44.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing I: 100%|██████████| 5/5 [00:00<00:00, 64.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing J\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing J: 100%|██████████| 13681/13681 [04:52<00:00, 46.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing K: 100%|██████████| 3935/3935 [01:17<00:00, 51.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing L: 100%|██████████| 9927/9927 [03:32<00:00, 46.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing M: 100%|██████████| 7959/7959 [02:46<00:00, 47.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing N: 100%|██████████| 4104/4104 [01:21<00:00, 50.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing O: 100%|██████████| 232/232 [00:03<00:00, 58.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing P: 100%|██████████| 3501/3501 [01:08<00:00, 51.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing Q: 100%|██████████| 6602/6602 [02:15<00:00, 48.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing R: 100%|██████████| 2869/2869 [00:53<00:00, 53.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing S: 100%|██████████| 17603/17603 [06:22<00:00, 45.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing T: 100%|██████████| 7586/7586 [02:37<00:00, 48.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing U: 100%|██████████| 2/2 [00:00<00:00, 61.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing V: 100%|██████████| 2/2 [00:00<00:00, 121.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing W: 100%|██████████| 6483/6483 [02:14<00:00, 48.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing X: 100%|██████████| 13039/13039 [04:34<00:00, 47.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing Y: 100%|██████████| 14842/14842 [05:08<00:00, 48.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing Z: 100%|██████████| 13476/13476 [04:24<00:00, 50.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved.\n",
      "saved.\n",
      "saved.\n",
      "img saved.\n",
      "img saved.\n",
      "img saved.\n",
      "totally 951089 images, avg image num of 151775 concept = 6.2664404546203265\n",
      "  total_images = 951089\n",
      "            total_concepts = 151775\n",
      "            total_descriptions(how many senses) = 158186 \n",
      "            total_multi_sense = 5854\n",
      "            total_description_len = 16614523\n",
      "            avg_description_len = 105.0315641080753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import chain\n",
    "from pypinyin import pinyin, Style\n",
    "# utils: 汉字转拼音\n",
    "def to_pinyin(s):\n",
    "    return ''.join(chain.from_iterable(pinyin(s, style=Style.TONE3)))\n",
    "\n",
    "def load_concept_images_and_descriptions(concept_name):\n",
    "    \n",
    "    first_pinyin = to_pinyin(concept_name)[0].upper()\n",
    "    grounded_concepts_path = os.path.join(grounded_concept_path, first_pinyin + \"/\" + concept_name + \".json\")\n",
    "    with open(grounded_concepts_path, 'r', encoding='utf-8') as f:\n",
    "        concept_dict = json.load(f)\n",
    "    # print(concept_dict)\n",
    "    image_description_pairs = []\n",
    "    for concept in concept_dict:\n",
    "        sampled_image_name = concept_dict[concept][\"concept_images\"][0]\n",
    "        # concept_images = concept_dict[concept][\"concept_images\"]\n",
    "        concept_description = concept_dict[concept][\"concept_description\"]\n",
    "        # for img_name in concept_images:\n",
    "        # img_path = os.path.join(weighted_image_path, first_pinyin + \"/\" + concept.split(\"_\")[0] + \"/\" + sampled_image_name)\n",
    "            # print(img_path)\n",
    "            # print(concept_description)\n",
    "        image_description_pairs.append((img_path, concept_description))\n",
    "    # image_description_pairs = image_description_pairs[:5] \n",
    "    # print(len(image_description_pairs))\n",
    "    return image_description_pairs\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os, json\n",
    "grounded_concept_path = \"./data_122/m2conceptbase/grounded_concepts/\"\n",
    "\n",
    "letter_list = [chr(ord('A') + i) for i in range(26)]\n",
    "\n",
    "concrete_concepts_dict = {}\n",
    "concrete_concepts_img_dict = {}\n",
    "abstract_concepts_dict = {}\n",
    "abstract_concepts_img_dict = {}\n",
    "extra_concepts_dict = {}\n",
    "extra_concepts_img_dict = {}\n",
    "\n",
    "\n",
    "grounded_concepts = set()\n",
    "total_images = 0\n",
    "total_concepts = 0\n",
    "total_descriptions = 0 # how many senses\n",
    "total_multi_sense = 0\n",
    "total_description_len = 0\n",
    "for letter in letter_list:\n",
    "    print(f\"processing {letter}\")\n",
    "    letter_dir_path = os.path.join(grounded_concept_path, letter)\n",
    "    concept_name_list = os.listdir(letter_dir_path)\n",
    "    for concept_name in tqdm(concept_name_list, total=len(concept_name_list), desc=f\"processing {letter}\"):\n",
    "        # print(concept_name)\n",
    "        concept_name = concept_name.replace(\".json\", \"\")\n",
    "        if concept_name in m2conceptbase_concept_label_dict:\n",
    "            # print(concept_name)\n",
    "            total_concepts += 1\n",
    "            with open(os.path.join(letter_dir_path, concept_name + \".json\"), 'r') as f:\n",
    "                grounded_concepts = json.load(f)\n",
    "                if len(grounded_concepts) > 1:\n",
    "                    total_multi_sense += 1\n",
    "                for concept in grounded_concepts:\n",
    "                    total_images += len(grounded_concepts[concept][\"concept_images\"])\n",
    "                    total_descriptions += 1\n",
    "                    total_description_len += len(grounded_concepts[concept][\"concept_description\"])\n",
    "                    # concrete_concepts\n",
    "                    if concept_name in concrete_concepts:\n",
    "                        if concept_name in concrete_concepts_dict:\n",
    "                            # print(concrete_concepts_dict)\n",
    "                            concrete_concepts_dict[concept_name].append(grounded_concepts[concept][\"concept_description\"])\n",
    "                        else:\n",
    "                            concrete_concepts_dict[concept_name] = []\n",
    "                            concrete_concepts_dict[concept_name].append(grounded_concepts[concept][\"concept_description\"])\n",
    "                        \n",
    "                        if concept_name in concrete_concepts_img_dict:\n",
    "                            concrete_concepts_img_dict[concept_name].append(grounded_concepts[concept][\"concept_images\"])\n",
    "                        else:\n",
    "                            concrete_concepts_img_dict[concept_name] = []\n",
    "                            concrete_concepts_img_dict[concept_name].append(grounded_concepts[concept][\"concept_images\"])\n",
    "                    # abstract_concepts\n",
    "                    if concept_name in abstract_concepts:\n",
    "                        if concept_name in abstract_concepts_dict:\n",
    "                            abstract_concepts_dict[concept_name].append(grounded_concepts[concept][\"concept_description\"])\n",
    "                        else:\n",
    "                            abstract_concepts_dict[concept_name] = []\n",
    "                            abstract_concepts_dict[concept_name].append(grounded_concepts[concept][\"concept_description\"])\n",
    "                    \n",
    "                    if concept_name in abstract_concepts:\n",
    "                        if concept_name in abstract_concepts_img_dict:\n",
    "                            abstract_concepts_img_dict[concept_name].append(grounded_concepts[concept][\"concept_images\"])\n",
    "                        else:\n",
    "                            abstract_concepts_img_dict[concept_name] = []\n",
    "                            abstract_concepts_img_dict[concept_name].append(grounded_concepts[concept][\"concept_images\"])\n",
    "                    # extra_concepts\n",
    "                    if concept_name in extra_concepts:\n",
    "                        if concept_name in extra_concepts_dict:\n",
    "                            extra_concepts_dict[concept_name].append(grounded_concepts[concept][\"concept_description\"])\n",
    "                        else:\n",
    "                            extra_concepts_dict[concept_name] = []\n",
    "                            extra_concepts_dict[concept_name].append(grounded_concepts[concept][\"concept_description\"])\n",
    "                    \n",
    "                    if concept_name in extra_concepts:\n",
    "                        if concept_name in extra_concepts_img_dict:\n",
    "                            extra_concepts_img_dict[concept_name].append(grounded_concepts[concept][\"concept_images\"])\n",
    "                        else:\n",
    "                            extra_concepts_img_dict[concept_name] = []\n",
    "                            extra_concepts_img_dict[concept_name].append(grounded_concepts[concept][\"concept_images\"])\n",
    "                    \n",
    "    # grounded_concepts = grounded_concepts | set(concept_name_list)\n",
    "# grounded_concepts = {concept.replace(\".json\", \"\") for concept in grounded_concepts}\n",
    "# print(len(grounded_concepts)) # 185070\n",
    "with open(\"./m2conceptbase/concrete_concepts_descriptions.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(concrete_concepts_dict, f, ensure_ascii=False, indent=4)\n",
    "print(\"saved.\")\n",
    "with open(\"./m2conceptbase/abstract_concepts_descriptions.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(abstract_concepts_dict, f, ensure_ascii=False, indent=4)\n",
    "print(\"saved.\")\n",
    "with open(\"./m2conceptbase/extra_concepts_descriptions.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(extra_concepts_dict, f, ensure_ascii=False, indent=4)\n",
    "print(\"saved.\")\n",
    "\n",
    "# save img\n",
    "with open(\"./m2conceptbase/concrete_concepts_images.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(concrete_concepts_img_dict, f, ensure_ascii=False, indent=4)\n",
    "print(\"img saved.\")\n",
    "with open(\"./m2conceptbase/abstract_concepts_images.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(abstract_concepts_img_dict, f, ensure_ascii=False, indent=4)\n",
    "print(\"img saved.\")\n",
    "with open(\"./m2conceptbase/extra_concepts_images.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(extra_concepts_img_dict, f, ensure_ascii=False, indent=4)\n",
    "print(\"img saved.\")\n",
    "\n",
    "print(f\"totally {total_images} images, avg image num of {total_concepts} concept = {total_images/total_concepts}\")\n",
    "print(f\"\"\"  total_images = {total_images}\n",
    "            total_concepts = {total_concepts}\n",
    "            total_descriptions(how many senses) = {total_descriptions} \n",
    "            total_multi_sense = {total_multi_sense}\n",
    "            total_description_len = {total_description_len}\n",
    "            avg_description_len = {total_description_len / total_descriptions}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "concrete_concepts_descriptions = \"./data/m2conceptbase/concrete_concepts_descriptions.json\"\n",
    "\n",
    "concrete_concepts_descriptions = \"./data/m2conceptbase/extra_concepts_descriptions.json\"\n",
    "texts = []\n",
    "with open(concrete_concepts_descriptions, 'r', encoding='utf-8') as f:\n",
    "    text = json.load(f) \n",
    "    for c, text_list in text.items():\n",
    "        texts.extend(text_list)\n",
    "    # print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 示例文本数据\n",
    "# texts = [\n",
    "#     \"这是一篇关于科技的文章\",\n",
    "#     \"这是一篇关于健康的文章\",\n",
    "#     # ... （其余的文本数据）\n",
    "# ]\n",
    "\n",
    "# 加载预训练的BERT模型和分词器\n",
    "model_name = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 将模型和数据移动到GPU设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"对文本数据进行分词..\")\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=128, padding='max_length', truncation=True) for text in texts]).to(device)\n",
    "\n",
    "# 对文本数据进行编码\n",
    "print(\"对文本数据进行编码..\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs.last_hidden_state  # 获取BERT的Word Embeddings\n",
    "\n",
    "# 将Word Embeddings降维至二维空间以便可视化\n",
    "print(\"将Word Embeddings降维至二维空间以便可视化..\")\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=10)  # 使用较小的perplexity\n",
    "embeddings_2d = tsne.fit_transform(embeddings[:, 0, :].cpu().numpy())  # 取第一个token的embedding\n",
    "\n",
    "# 可视化主题热图\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(embeddings_2d, cmap='viridis', aspect='auto')\n",
    "plt.title('BERT Word Embeddings 主题热图')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# 计算文本间的相似性矩阵\n",
    "similarity_matrix = np.inner(embeddings[:, 0, :].cpu().numpy(), embeddings[:, 0, :].cpu().numpy())\n",
    "\n",
    "# 可视化主题分布图\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(similarity_matrix, cmap='viridis', aspect='auto')\n",
    "plt.title('BERT Word Embeddings 主题分布图')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pyLDAvis\n",
    "# import pyLDAvis.sklearn\n",
    "import pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "concrete_concepts_descriptions = \"./data/m2conceptbase/concrete_concepts_descriptions.json\"\n",
    "\n",
    "concrete_concepts_descriptions = \"./data/m2conceptbase/extra_concepts_descriptions.json\"\n",
    "texts = []\n",
    "with open(concrete_concepts_descriptions, 'r', encoding='utf-8') as f:\n",
    "    text = json.load(f) \n",
    "    for c, text_list in text.items():\n",
    "        texts.extend(text_list)\n",
    "    # print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(texts)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=20, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=20, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=20, random_state=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda_tf.fit(dtm_tf)\n",
    "\n",
    "# TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)\n",
    "\n",
    "pyLDAvis.lda_model.prepare(lda_model=lda_tf, dtm=dtm_tf, vectorizer=tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "def to_jl(file_t):\n",
    "    with open('/mnt/zhaohaiquan/test/Chinese-CLIP/data/datasets/SMD/zheng3.json','r',encoding='utf-8') as f:\n",
    "        know_data=json.load(f)\n",
    "    f.close()\n",
    "    num=0\n",
    "    id_set=set()\n",
    "    train=pd.read_csv(file_t)\n",
    "    for tem in train['label']:\n",
    "        id_set.add(str(tem))\n",
    "    fault_list=[]\n",
    "    for file in tqdm(os.listdir(\"./image_final/\"),desc='read img:'):\n",
    "        if(\"_\" in file):\n",
    "            continue\n",
    "        elif(file.replace(\".jpg\",\"\") not in id_set):\n",
    "            fault_list.append(file.replace('.jpg',''))\n",
    "        else:\n",
    "            try:\n",
    "                img = Image.open('./image_final/'+file) # 访问图片路径\n",
    "            except:\n",
    "                print(file)\n",
    "                fault_list.append(file.replace('.jpg',''))\n",
    "    with open('./SMD1/'+file_t.replace('.csv',\".jsonl\"),'w',encoding='utf-8') as w:\n",
    "        for id in tqdm(id_set,desc='json:'):\n",
    "            if(id in fault_list or id not in know_data.keys()):\n",
    "                continue\n",
    "            else:\n",
    "                tem_dict={}\n",
    "                tem_dict[\"text_id\"]= int(id)\n",
    "                if('lv3' in know_data[id].keys()):\n",
    "                    text=know_data[id][\"commodity_name\"]+'/'+know_data[id]['lv3']+'/'+know_data[id]['lv2']+'/'+know_data[id]['lv1']+'/'+know_data[id]['lv0']\n",
    "                else:\n",
    "                    text=know_data[id][\"commodity_name\"]+'/'+know_data[id]['lv2']+'/'+know_data[id]['lv1']+'/'+know_data[id]['lv0']\n",
    "                tem_dict[\"text\"]=text\n",
    "                tem_list=[]\n",
    "                tem_list.append(id)\n",
    "                num+=1\n",
    "                for index,tem in enumerate(train['label']):\n",
    "                    if(str(tem)==id):\n",
    "                        tem_list.append(train['image'][index].replace(\".jpg\",\"\")) \n",
    "                        num+=1\n",
    "                tem_dict[\"image_ids\"]=tem_list\n",
    "                json_str=json.dumps(tem_dict,ensure_ascii=False)\n",
    "                w.write(json_str+\"\\n\")\n",
    "    w.close()\n",
    "    print(num)\n",
    "\n",
    "def to_tsv():\n",
    "    num=0\n",
    "    i=0\n",
    "    df_train=pd.read_csv('data.csv')\n",
    "    with open('/mnt/zhaohaiquan/test/Chinese-CLIP/data/datasets/SMD/zheng3.json','r',encoding='utf-8') as f:\n",
    "        know_data=json.load(f)\n",
    "    f.close()\n",
    "    df_val=pd.read_csv('val.csv')\n",
    "    train_list=[]\n",
    "    val_list=[]\n",
    "    for tem in df_val['label']:\n",
    "        if(str(tem) in know_data.keys()):\n",
    "            val_list.append(str(tem))\n",
    "    for key in know_data.keys():\n",
    "        if key not in val_list:\n",
    "            train_list.append(key)\n",
    "    train_img=[]\n",
    "    val_img=[]\n",
    "    for tem in train_list:\n",
    "        train_img.append(tem+'.jpg')\n",
    "    for tem in val_list:\n",
    "        val_img.append(tem+'.jpg')\n",
    "    img_list=os.listdir(\"./image_final/\")\n",
    "    train_img_base64=[]\n",
    "    val_img_base64=[]\n",
    "    # fault_list=[]\n",
    "    # for img_url in tqdm(train_img,desc='fault_list'):\n",
    "    #     if(img_url in img_list):\n",
    "    #         if(\"_\" in img_url):\n",
    "    #             continue\n",
    "    #         else:\n",
    "    #             try:\n",
    "    #                 img = Image.open('./image_final/'+img_url) # 访问图片路径\n",
    "    #             except:\n",
    "    #                 num+=1\n",
    "    #                 fault_list.append(img_url)\n",
    "    # for tem in fault_list:\n",
    "    #     train_img.remove(tem)\n",
    "    #     val_image.remove(tem)\n",
    "    for index,tem in enumerate(tqdm(df_train['image'],desc='train')):\n",
    "        if(str(df_train['label'][index]) in train_list):\n",
    "            train_img.append(tem)\n",
    "        if(str(df_train['label'][index]) in val_list):\n",
    "            val_img.append(tem)\n",
    "    train_imgs=[]\n",
    "    val_imgs=[]\n",
    "    for img_url in tqdm(train_img,desc='base64'):\n",
    "        if(img_url in img_list):\n",
    "            train_imgs.append(img_url.replace(\".jpg\",\"\"))\n",
    "            img = Image.open('./image_final/'+img_url) # 访问图片路径\n",
    "            img_buffer = BytesIO()\n",
    "            img.save(img_buffer, format=img.format)\n",
    "            byte_data = img_buffer.getvalue()\n",
    "            base64_str = base64.b64encode(byte_data) # bytes\n",
    "            base64_str = base64_str.decode(\"utf-8\")\n",
    "            train_img_base64.append(base64_str)\n",
    "        else:\n",
    "            print(img_url)\n",
    "    for img_url in tqdm(val_img,desc='base64'):\n",
    "        if(img_url in img_list):\n",
    "            val_imgs.append(img_url.replace(\".jpg\",\"\"))\n",
    "            img = Image.open('./image_final/'+img_url) # 访问图片路径\n",
    "            img_buffer = BytesIO()\n",
    "            img.save(img_buffer, format=img.format)\n",
    "            byte_data = img_buffer.getvalue()\n",
    "            base64_str = base64.b64encode(byte_data) # bytes\n",
    "            base64_str = base64_str.decode(\"utf-8\")\n",
    "            val_img_base64.append(base64_str)\n",
    "        else:\n",
    "            print(img_url)\n",
    "    dft=pd.DataFrame({'id':train_imgs,\"base64\":train_img_base64})\n",
    "    dfv=pd.DataFrame({'id':val_imgs,\"base64\":val_img_base64})\n",
    "    print(i)\n",
    "    print(num)\n",
    "    dft.to_csv(\"./SMD1/train.tsv\",header=None,index=None,sep='\\t')\n",
    "    dfv.to_csv(\"./SMD1/val.tsv\",header=None,index=None,sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zzw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
